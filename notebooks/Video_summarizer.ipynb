{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFntEgYMIhIG"
      },
      "outputs": [],
      "source": [
        "# Install the required Python dependencies.\n",
        "!pip install -q omegaconf==2.3.0 iopath==0.1.10 timm==0.6.13 webdataset==0.2.48\n",
        "!pip install -q visual-genome==1.1.1 decord==0.6.0 pysrt==1.1.2 wandb==0.14.2\n",
        "!pip install -q openai==0.28.0 pycocoevalcap==1.2 pytubefix moviepy==1.0.3\n",
        "!pip install -q soundfile==0.12.1 sentencepiece==0.1.97 opencv-python==4.7.0.72\n",
        "!pip install -q scikit-image==0.22.0 webvtt-py==0.4.6 torch==2.0.1 triton==2.0.0\n",
        "!pip install -q torchaudio==2.0.2 torchvision==0.15.2 transformers==4.37.2 tokenizers==0.15.2\n",
        "!pip install -q bitsandbytes==0.42.0 peft==0.2.0 accelerate==0.25.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2aWMyQvIj46"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.26.3\n",
        "import numpy as np\n",
        "print(np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUS5Z0xDIm5g"
      },
      "outputs": [],
      "source": [
        "!pip install -q nltk\n",
        "import nltk\n",
        "nltk.download('wordnet', download_dir='/root/nltk_data')\n",
        "nltk.data.path.append('/root/nltk_data')\n",
        "!huggingface-cli login\n",
        "# use your own huggingface authentication key and  paste it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6n5Q-gHJVFQ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/camenduru/MiniGPT4-video\n",
        "%cd MiniGPT4-video\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \\\n",
        "  https://huggingface.co/Vision-CAIR/MiniGPT4-Video/resolve/main/checkpoints/video_llama_checkpoint_last.pth \\\n",
        "  -d /content/MiniGPT4-video/pretrained_models -o video_llama_checkpoint_last.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwaR67_bJX4X"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# === BACKEND CODE: Model initialization and utility functions ===\n",
        "import os\n",
        "import torch\n",
        "import webvtt\n",
        "import cv2\n",
        "from minigpt4.common.eval_utils import init_model\n",
        "from minigpt4.conversation.conversation import CONV_VISION\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import soundfile as sf\n",
        "import moviepy.editor as mp\n",
        "from PIL import Image\n",
        "from moviepy.editor import VideoFileClip\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.backends.cudnn as cudnn\n",
        "import yaml\n",
        "\n",
        "class Options:\n",
        "    def __init__(self):\n",
        "        self.cfg_path = \"/content/MiniGPT4-video/test_configs/llama2_test_config.yaml\"\n",
        "        self.ckpt = '/content/MiniGPT4-video/pretrained_models/video_llama_checkpoint_last.pth'\n",
        "        self.add_subtitles = False  # Set to True if you want to generate subtitles via whisper.\n",
        "        self.question = None\n",
        "        self.video_path = None\n",
        "        self.max_new_tokens = 512\n",
        "        self.lora_r = 64\n",
        "        self.lora_alpha = 16\n",
        "        self.options = None\n",
        "\n",
        "args = Options()\n",
        "\n",
        "def prepare_input(vis_processor, video_path, subtitle_path, instruction):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if subtitle_path is not None:\n",
        "        vtt_file = webvtt.read(subtitle_path)\n",
        "        print(\"Subtitle loaded successfully.\")\n",
        "        clip = VideoFileClip(video_path)\n",
        "        total_num_frames = int(clip.duration * clip.fps)\n",
        "        clip.close()\n",
        "    else:\n",
        "        total_num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    max_images_length = 45\n",
        "    max_sub_len = 400\n",
        "    images = []\n",
        "    frame_count = 0\n",
        "    sampling_interval = int(total_num_frames / max_images_length)\n",
        "    if sampling_interval == 0:\n",
        "        sampling_interval = 1\n",
        "    img_placeholder = \"\"\n",
        "    subtitle_text_in_interval = \"\"\n",
        "    history_subtitles = {}\n",
        "    raw_frames = []\n",
        "    number_of_words = 0\n",
        "    transform = transforms.Compose([transforms.ToPILImage(),])\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if subtitle_path is not None:\n",
        "            for subtitle in vtt_file:\n",
        "                sub = subtitle.text.replace('\\n',' ')\n",
        "                if (subtitle.start_in_seconds <= (frame_count / int(clip.fps)) <= subtitle.end_in_seconds) and sub not in subtitle_text_in_interval:\n",
        "                    if not history_subtitles.get(sub, False):\n",
        "                        subtitle_text_in_interval += sub + \" \"\n",
        "                    history_subtitles[sub] = True\n",
        "                    break\n",
        "        if frame_count % sampling_interval == 0:\n",
        "            raw_frames.append(Image.fromarray(cv2.cvtColor(frame.copy(), cv2.COLOR_BGR2RGB)))\n",
        "            frame = transform(frame[:,:,::-1])\n",
        "            frame = vis_processor(frame)\n",
        "            images.append(frame)\n",
        "            img_placeholder += '<Img><ImageHere>'\n",
        "            if subtitle_path is not None and subtitle_text_in_interval != \"\" and number_of_words < max_sub_len:\n",
        "                img_placeholder += f'<Cap>{subtitle_text_in_interval}'\n",
        "                number_of_words += len(subtitle_text_in_interval.split(' '))\n",
        "                subtitle_text_in_interval = \"\"\n",
        "        frame_count += 1\n",
        "        if len(images) >= max_images_length:\n",
        "            break\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    if len(images) == 0:\n",
        "        return None, None\n",
        "    images = torch.stack(images)\n",
        "    instruction = img_placeholder + '\\n' + instruction\n",
        "    return images, instruction\n",
        "\n",
        "def extract_audio(video_path, audio_path):\n",
        "    video_clip = mp.VideoFileClip(video_path)\n",
        "    audio_clip = video_clip.audio\n",
        "    audio_clip.write_audiofile(audio_path, codec=\"libmp3lame\", bitrate=\"320k\")\n",
        "\n",
        "def generate_subtitles(video_path):\n",
        "    video_id = video_path.split('/')[-1].split('.')[0]\n",
        "    audio_path = f\"workspace/inference_subtitles/mp3/{video_id}.mp3\"\n",
        "    os.makedirs(\"workspace/inference_subtitles/mp3\", exist_ok=True)\n",
        "    if existed_subtitles.get(video_id, False):\n",
        "        return f\"workspace/inference_subtitles/{video_id}.vtt\"\n",
        "    try:\n",
        "        extract_audio(video_path, audio_path)\n",
        "        print(\"Audio extracted successfully.\")\n",
        "        os.system(f\"whisper {audio_path} --language English --model large --output_format vtt --output_dir workspace/inference_subtitles\")\n",
        "        os.system(f\"rm {audio_path}\")\n",
        "        print(\"Subtitle generated successfully.\")\n",
        "        return f\"workspace/inference_subtitles/{video_id}.vtt\"\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        print(\"Error processing\", video_path)\n",
        "        return None\n",
        "\n",
        "def run(video_path, instruction, model, vis_processor, gen_subtitles=True):\n",
        "    if gen_subtitles:\n",
        "        subtitle_path = generate_subtitles(video_path)\n",
        "    else:\n",
        "        subtitle_path = None\n",
        "    prepared_images, prepared_instruction = prepare_input(vis_processor, video_path, subtitle_path, instruction)\n",
        "    if prepared_images is None:\n",
        "        return \"Video cannot be opened. Please check the video file.\"\n",
        "    length = len(prepared_images)\n",
        "    prepared_images = prepared_images.unsqueeze(0)\n",
        "    conv = CONV_VISION.copy()\n",
        "    conv.system = \"\"\n",
        "    conv.append_message(conv.roles[0], prepared_instruction)\n",
        "    conv.append_message(conv.roles[1], None)\n",
        "    prompt = [conv.get_prompt()]\n",
        "    answers = model.generate(prepared_images, prompt, max_new_tokens=args.max_new_tokens,\n",
        "                               do_sample=True, lengths=[length], num_beams=1)\n",
        "    return answers[0]\n",
        "\n",
        "def setup_seeds(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    cudnn.benchmark = False\n",
        "    cudnn.deterministic = True\n",
        "\n",
        "with open('test_configs/llama2_test_config.yaml') as file:\n",
        "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
        "seed = config['run']['seed']\n",
        "print(\"Seed:\", seed)\n",
        "\n",
        "# Initialize the model and visual processor.\n",
        "model, vis_processor = init_model(args)\n",
        "conv = CONV_VISION.copy()\n",
        "conv.system = \"\"\n",
        "inference_subtitles_folder = \"inference_subtitles\"\n",
        "os.makedirs(inference_subtitles_folder, exist_ok=True)\n",
        "existed_subtitles = {}\n",
        "for sub in os.listdir(inference_subtitles_folder):\n",
        "    existed_subtitles[sub.split('.')[0]] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BV9MZ-5dJY3Q"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "# === GRADIO INTERFACE: Define UI and connect backend ===\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def process_video(video, question):\n",
        "    if video is None:\n",
        "        return \"Please upload a video file.\"\n",
        "    # Gradio may provide the video as a filepath or a file object.\n",
        "    video_path = video if isinstance(video, str) else video.name\n",
        "    result = run(video_path, question, model, vis_processor, gen_subtitles=args.add_subtitles)\n",
        "    return result\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_video,\n",
        "    inputs=[\n",
        "        gr.Video(label=\"Upload Video\"),\n",
        "        gr.Textbox(label=\"Enter your question\", placeholder=\"Question\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"â€œVideo Understanding and Language-based Contextual Answer Network\",\n",
        "    description=\"Upload a video and ask any question related to it. The model processes the video and generates an answer.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}